{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, cuda, optim\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_df = pd.read_csv(filepath_or_buffer=\"../data/train.csv\")\n",
    "testing_input_df = pd.read_csv(filepath_or_buffer=\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/byyvclnj29l_3mpjgqsq49yc0000gp/T/ipykernel_39679/3930669996.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n",
      "/var/folders/sp/byyvclnj29l_3mpjgqsq49yc0000gp/T/ipykernel_39679/3930669996.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n",
      "/var/folders/sp/byyvclnj29l_3mpjgqsq49yc0000gp/T/ipykernel_39679/3930669996.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n",
      "/var/folders/sp/byyvclnj29l_3mpjgqsq49yc0000gp/T/ipykernel_39679/3930669996.py:8: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  text = text.str.replace(\"\\s{2,}\", \" \")\n"
     ]
    }
   ],
   "source": [
    "# drop 'id' , 'keyword' and 'location' columns.\n",
    "training_input_df.drop(columns=['id','keyword','location'], inplace=True)\n",
    "training_input_df[\"text\"]=normalise_text(training_input_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: DeprecationWarning: invalid escape sequence '\\s'\n",
      "<>:8: DeprecationWarning: invalid escape sequence '\\s'\n",
      "/var/folders/sp/byyvclnj29l_3mpjgqsq49yc0000gp/T/ipykernel_39679/3930669996.py:8: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  text = text.str.replace(\"\\s{2,}\", \" \")\n"
     ]
    }
   ],
   "source": [
    "# to clean data\n",
    "def normalise_text (text):\n",
    "    text = text.str.lower() # lowercase\n",
    "    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n",
    "    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n",
    "    text = text.str.replace(r\"@\",\"\")\n",
    "    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n",
    "    text = text.str.replace(\"\\s{2,}\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_df.head()\n",
    "# split data into train and validation \n",
    "train_df, valid_df = train_test_split(training_input_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialiaze the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base') # or any other tokenizer\n",
    "sentences = training_input_df[\"text\"]\n",
    "sentences_tokenized = [tokenizer.tokenize(sentence) for sentence in sentences ]\n",
    "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence0))\n",
    "# print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = torchtext.data.get_tokenizer(\"basic_english\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.data = [] # (sent1, label1) ...\n",
    "        for i, row in dataframe.iterrows():\n",
    "            text, target = row['text'], row['target']\n",
    "            self.data.append((text, int(target)))\n",
    "            self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, target = self.data[idx]\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=76, return_tensors='pt')\n",
    "        inputs_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_masks = inputs['attention_mask'].squeeze(0)\n",
    "        return inputs_ids, attention_masks, target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_df, tokenizer)\n",
    "val_dataset = CustomDataset(valid_df, tokenizer)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "# inputs_ids, attention_masks, label = next(iter(train_dataloader))\n",
    "# print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 501M/501M [00:27<00:00, 18.0MB/s] \n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# model that we keep for validation\n",
    "best_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base', # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device to run the model on (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell pytorch to run this model on the GPU.\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srijon1/miniforge3/envs/local_nmt/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW (model.parameters(),\n",
    "                  lr =1e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
    "                  weight_decay=0.05\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 50\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 50\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].plot(train_losses, label='Train Loss')\n",
    "    axs[0].plot(val_losses, label='Validation Loss')\n",
    "    axs[0].set_title(\"Losses over Epochs\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(train_accuracies, label='Train Accuracy')\n",
    "    axs[1].plot(val_accuracies, label='Validation Accuracy')\n",
    "    axs[1].set_title(\"Accuracies over Epochs\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "#     plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def compute_acc(preds, labels):\n",
    "    correct = 0\n",
    "    preds_ = preds.data.max(1)[1]\n",
    "    correct = preds_.eq(labels.data).cpu().sum()\n",
    "    acc = float(correct) / float(len(labels.data)) * 100.0\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_scalar(t_tensor, s):\n",
    "    return t_tensor.eq(torch.from_numpy(np.array([s])).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_stats(predictions, labels):\n",
    "    true_positives = torch.sum(torch.logical_and(compare_scalar(labels, 1), compare_scalar(predictions, 1)).int()).item()\n",
    "    false_positives = torch.sum(torch.logical_and(compare_scalar(labels, 0), compare_scalar(predictions, 1)).int()).item()\n",
    "    false_negatives = torch.sum(torch.logical_and(compare_scalar(labels, 1), compare_scalar(predictions, 0)).int()).item()\n",
    "    true_negatives = torch.sum(torch.logical_and(compare_scalar(labels, 0), compare_scalar(predictions, 0)).int()).item()\n",
    "    return true_positives, false_positives, false_negatives, true_negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! alternative and prefered evals\n",
    "def eval_model(model, loader):\n",
    "    # Keep track of the total loss for the batch\n",
    "    print(\"Evaluating Model\")\n",
    "    true_positives, false_positives, false_negatives, true_negatives = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for a in tqdm(loader):\n",
    "            input_ids = torch.from_numpy(np.array(a[0])).to(device)\n",
    "            masks = torch.from_numpy(np.array(a[1])).to(device)\n",
    "            labels = torch.from_numpy(np.array(a[2])).to(device)\n",
    "            output = model(input_ids, masks)\n",
    "            predictions = torch.argmax(output, dim=1).int()\n",
    "            tp, fp, fn, tn = get_accuracy_stats(predictions, labels)\n",
    "            true_positives += tp\n",
    "            false_positives += fp\n",
    "            false_negatives += fn\n",
    "            true_negatives += tn\n",
    "\n",
    "    f1_score = true_positives / (true_positives + 0.5 *(false_positives + false_negatives))\n",
    "    accuracy = (true_positives+true_negatives) / (true_positives + false_positives + false_negatives + true_negatives) * 100\n",
    "    return accuracy, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 50 ========\n",
      "Training...\n",
      "  Batch 20 / 179.  Elapsed: 0:01:35.\n",
      "Training loss: 0.689  Training acc: 56.094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m# This is to help prevent the \"exploding gradients\" problem.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), \u001b[39m1.0\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     77\u001b[0m \u001b[39m# Update the learning rate.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/local_nmt/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/local_nmt/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniforge3/envs/local_nmt/lib/python3.10/site-packages/transformers/optimization.py:360\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    356\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    358\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m(\u001b[39m1.0\u001b[39;49m \u001b[39m-\u001b[39;49m beta1))\n\u001b[1;32m    361\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    362\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39msqrt()\u001b[39m.\u001b[39madd_(group[\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "patience = 4 \n",
    "num_no_improvement = 0 \n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "show_every = 20\n",
    "\n",
    "# Keep track of the best validation loss\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    store_train_loss = []\n",
    "    store_train_acc = []\n",
    "    store_val_loss = []\n",
    "    store_val_acc = []\n",
    "    \n",
    "     # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    \n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs_ids, attention_masks, labels = batch\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(inputs_ids, \n",
    "                    attention_mask=attention_masks)\n",
    "        \n",
    "#         print(outputs[0])\n",
    "#         print(labels.shape)\n",
    "#         print(outputs[0].shape)\n",
    "        loss = F.cross_entropy(outputs[0], labels)\n",
    "#         print(outputs.shape)\n",
    "        \n",
    "        train_acc = compute_acc(outputs[0], labels)\n",
    "        \n",
    "\n",
    "        # Print the statistics\n",
    "        store_train_loss.append(loss.item())\n",
    "        store_train_acc.append(train_acc)\n",
    "        \n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Progress update every 40 batches.\n",
    "        if step % show_every == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {} / {}.  Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print('Training loss: %.3f  Training acc: %.3f'%(np.mean(store_train_loss[-show_every:]), np.mean(store_train_acc[-show_every:])) ) \n",
    "    \n",
    "        \n",
    "    # compute epoch loss and accuracy \n",
    "    train_losses.append(np.mean(store_train_loss))\n",
    "    train_accuracies.append(np.mean(store_train_acc))\n",
    "    \n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate data for one epoch\n",
    "    for batch in val_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        val_inputs_ids, val_attention_masks, val_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():  \n",
    "            val_outputs = model(val_inputs_ids,  \n",
    "                            attention_mask=val_attention_masks)\n",
    "            \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        val_logits = val_outputs[0]\n",
    "        \n",
    "        val_loss = F.cross_entropy(val_logits, val_labels)\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        val_acc = compute_acc(val_logits, val_labels)\n",
    "        \n",
    "        store_val_loss.append(val_loss.item())\n",
    "        store_val_acc.append(val_acc)\n",
    "        \n",
    "    \n",
    "    # compute epoch loss and accuracy \n",
    "    mean_val_loss = np.mean(store_val_loss)\n",
    "    val_losses.append(mean_val_loss)\n",
    "    val_accuracies.append(np.mean(store_val_acc))\n",
    "        \n",
    "    # Report the final accuracy for this validation run.\n",
    "    # Print loss and acc at the end of the epoch\n",
    "    print(\"Epoch {}: Train Loss: {:.4f}, Validation Loss: {:.4f}, Train Accuracy: {:.2f}%, Validation Accuracy: {:.2f}%\".format\n",
    "    (epoch_i, train_losses[-1], val_losses[-1], train_accuracies[-1], val_accuracies[-1]))\n",
    "    \n",
    "\n",
    "\n",
    "    #Check if validation loss has improved\n",
    "    if np.mean(mean_val_loss < best_val_loss):\n",
    "        best_val_loss = mean_val_loss\n",
    "        # Get the current state of the model\n",
    "        model_state_dict = model.state_dict()\n",
    "        best_model.load_state_dict(model_state_dict)\n",
    "        num_no_improvement = 0 \n",
    "    else:\n",
    "        num_no_improvement+=1 \n",
    "    if num_no_improvement == patience: \n",
    "        break\n",
    "\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "test.drop(columns=['keyword','location'], inplace=True)\n",
    "\n",
    "test[\"text\"]=normalise_text(test[\"text\"])\n",
    "\n",
    "test['target'] = 0\n",
    "\n",
    "test_dataset = CustomDataset(test, tokenizer)\n",
    "batch_size = 8\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state dictionary into the model\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "\n",
    "store_test_acc = []\n",
    "\n",
    "store_predictions = torch.zeros(len(test_dataset), dtype=torch.int)\n",
    "\n",
    "# For each batch of training data...\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    \n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs_ids, attention_masks, labels = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = best_model(inputs_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=attention_masks)\n",
    "        predictions = torch.max(outputs[0], dim=1)[1]\n",
    "        store_predictions[step*len(predictions): (step+1)*len(predictions)] = predictions\n",
    "        # store_test_acc.append(compute_acc(outputs[0], labels))\n",
    "\n",
    "\n",
    "store_predictions = store_predictions.detach().cpu().numpy()\n",
    "\n",
    "test['target'] = store_predictions\n",
    "\n",
    "test.drop(columns=['text'], inplace=True)\n",
    "\n",
    "test.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Finished testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_nmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
